{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8f7bf7",
   "metadata": {},
   "source": [
    "## 1.Introduction to PyTorch, a Deep Learning Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062a44e",
   "metadata": {},
   "source": [
    "### 1-1. Tensors\n",
    "- Similar to array or matrix\n",
    "- Building block of neural networks \n",
    "##\n",
    "- Can be added or substracted, provided that their shapes are compatible\n",
    "- a * b : element-wise multiplication vs a @ b = Matrix multiplication <br>\n",
    "\n",
    "        → Perform addition and multiplication to process data and learn patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[72, 75, 78],\n",
      "        [70, 73, 76]])\n"
     ]
    }
   ],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "\n",
    "temperatures = [[72, 75, 78], [70, 73, 76]] # can be created from Python lists or Numpy arrays\n",
    "\n",
    "# Create a tensor from temperatures\n",
    "temperatures= torch.tensor(temperatures)\n",
    "\n",
    "print(temperatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b4912",
   "metadata": {},
   "source": [
    "### 1-2. Cheking and adding tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0232f8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjustment shape: torch.Size([2, 3])\n",
      "Adjustment type: torch.int64\n",
      "Temperatures shape: torch.Size([2, 3])\n",
      "Temperatures type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "adjustment = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
    "\n",
    "# Display the shape of the adjustment tensor\n",
    "print(\"Adjustment shape:\", adjustment.shape)\n",
    "\n",
    "# Display the type of the adjustment tensor\n",
    "\n",
    "print(\"Adjustment type:\", adjustment.dtype)\n",
    "\n",
    "print(\"Temperatures shape:\", temperatures.shape)\n",
    "print(\"Temperatures type:\", temperatures.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27121fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected temperatures: tensor([[74, 77, 80],\n",
      "        [72, 75, 78]])\n"
     ]
    }
   ],
   "source": [
    "adjustment = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
    "\n",
    "# Add the temperatures and adjustment tensors\n",
    "corrected_temperatures = adjustment + temperatures\n",
    "print(\"Corrected temperatures:\", corrected_temperatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417216d",
   "metadata": {},
   "source": [
    "### 1-3. Neural networks and layers\n",
    "- A neural network consists of input(dataset features), hidden, and output layers(predictions).\n",
    "- ''Fully connected network''(= linear model) \n",
    "    - Every input neuron connects to every output neuron \n",
    "    - Network with no hidden layers where the output layer is a linear layer\n",
    "    - Linear layer : input @ wieghts + bias = output\n",
    "\n",
    "\n",
    "- weight : Reflects the importance of different features\n",
    "- bias : Provides the neuron with a baseline output \n",
    "\n",
    "ex) \n",
    "- input (temperature, humidity, wind) → output (rain, cloudy) <br>\n",
    "humidity feature will have a more significant weight<br>\n",
    "bias is to account for baseline information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f1cc483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3970,  0.1681]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Linear layer network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]])\n",
    "\n",
    "# Create a Linear layer \n",
    "linear_layer = nn.Linear (\n",
    "    in_features= 3,\n",
    "    out_features= 2\n",
    ")\n",
    "\n",
    "# Pass input_tensor through the linear layer \n",
    "output = linear_layer(input_tensor)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b39d6",
   "metadata": {},
   "source": [
    "### 1-4. Hidden layer and parameters\n",
    "- Stacking layes with nn.Seqential()\n",
    "\n",
    "Layer are made of neurons \n",
    "- Fully connected : each neuron links to all neurons in the previous layer \n",
    "- A neuron in a linear layer :\n",
    "    - perfroms a linear operation using all neurons from the previous layer \n",
    "    - Has N+1 parameters : N from inputs and 1 for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc831a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0686]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Create a container for stacking linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8,4),\n",
    "    nn.Linear(4,1)\n",
    ") # cf) nn.Linear(input features, output features)\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb439052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2275,  0.2435, -0.0524,  0.3298,  0.3296,  0.2640, -0.2381, -0.2140,\n",
      "          0.0264],\n",
      "        [-0.3031,  0.0515, -0.0308, -0.0951,  0.1180,  0.1785, -0.1220, -0.0733,\n",
      "          0.0759],\n",
      "        [-0.1972,  0.2063, -0.0784, -0.3155,  0.0543, -0.0286,  0.2107, -0.0309,\n",
      "         -0.1947],\n",
      "        [-0.2835,  0.2321,  0.0301, -0.2333,  0.2362,  0.2412,  0.0559, -0.1665,\n",
      "         -0.1575]], requires_grad=True)\n",
      "36\n",
      "Parameter containing:\n",
      "tensor([-0.0622,  0.3263, -0.0052, -0.0351], requires_grad=True)\n",
      "40\n",
      "Parameter containing:\n",
      "tensor([[-0.0515, -0.4406,  0.4247, -0.3446],\n",
      "        [-0.2672, -0.3528, -0.1264, -0.1666]], requires_grad=True)\n",
      "48\n",
      "Parameter containing:\n",
      "tensor([-0.2156, -0.3809], requires_grad=True)\n",
      "50\n",
      "Parameter containing:\n",
      "tensor([[-0.0717,  0.0155]], requires_grad=True)\n",
      "52\n",
      "Parameter containing:\n",
      "tensor([0.4634], requires_grad=True)\n",
      "53\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of parameters\n",
    "\n",
    "model = nn.Sequential(nn.Linear(9, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "total = 0\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)  # Weight & bias\n",
    "    total += parameter.numel()\n",
    "    print(total)\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3295ef",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d2363",
   "metadata": {},
   "source": [
    "## Week 1 review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157ca7a",
   "metadata": {},
   "source": [
    "###  Section 1: Implementing a simple linear NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ad360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) : \n",
    "        super(Net, self).__init__()  # Initalise parent class\n",
    "        self.fc1 = nn.Linear(2,1)\n",
    "\n",
    "    def forward(self, x):  # The function we specify which function to apply on the input \n",
    "        x = self.fc1(x)   # 입력값에 가중치 곱하고 편향 더해서 계산하는 것 \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6cd700d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.6601, -0.0942]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2797], requires_grad=True)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = Net()\n",
    "\n",
    "list(net1.parameters())\n",
    "\n",
    "# requires_grad=True : 자동 미분 속성 \n",
    "# → Backpropagation 알고리즘을 실행할 경우, Pytorch가 이 매개변수들을 최적화(학습)할 것임을 의미한다\n",
    "# → 매개변수를 최적화 하려면 손실 값 (loss value)에 따른 기울기를 저장해야하는데 이 때 이 키워드가 필요한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95ea0104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3928]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing whether nerons works\n",
    "x_input = torch.tensor([[0.1, 0.5]])\n",
    "net1(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebf863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.39281"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (0.1* - 0.6601) + (0.5 * -0.0942) + -0.2797"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db67ec",
   "metadata": {},
   "source": [
    "### Section 2: Implementing a perceptron in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c978a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e04d8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        # Gap 1: single linear unit\n",
    "        self.fc1 = nn.Linear(2, 1)\n",
    "        \n",
    "        # Hardcoded weights and bias\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight = nn.Parameter(torch.tensor([[1.0, 1.0]]))\n",
    "            self.fc1.bias   = nn.Parameter(torch.tensor([-1.5]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Gap 2: Heaviside step function\n",
    "        x = (x > 0).float()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fc577cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_perceptron = Perceptron()\n",
    "my_perceptron.fc1.weight.data = torch.tensor([[ 0.4, 0.2]])\n",
    "my_perceptron.fc1.bias.data = torch.tensor([-0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "393541e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input [0.0, 0.0] -> Output 0\n",
      "Input [0.0, 1.0] -> Output 0\n",
      "Input [1.0, 0.0] -> Output 1\n",
      "Input [1.0, 1.0] -> Output 1\n"
     ]
    }
   ],
   "source": [
    "test_inputs = torch.tensor([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0]\n",
    "])\n",
    "\n",
    "outputs = my_perceptron(test_inputs)\n",
    "\n",
    "for x, y in zip(test_inputs, outputs):\n",
    "    print(f\"Input {x.tolist()} -> Output {int(y.item())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8bf49",
   "metadata": {},
   "source": [
    "## Section 3: Solving a classification problem using a Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa46a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1)\n",
    "        \n",
    "        self.fc1.weights = torch.nn.Parameter(torch.tensor([[0.66],[1]])) # 둘다 그림에서 계산한거 \n",
    "        self.fc1.bias = torch.nn.Parameter(torch.tensor([[-0.5]]))  \n",
    "        \n",
    "        self.heaviside = torch.heaviside\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        output = self.heavside(x, 0.5)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92318461",
   "metadata": {},
   "source": [
    "## Section 4: Training a perceptron\n",
    "- implement a method that trains the modelsuing some training data + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3808d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(0.7,0.3,1),(0.4,0.5,1), (0.6,0.9, 1), (0.2,0.2, 0), (0.1, 0.1, 0)]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.heaviside(x, torch.Tensor(1))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf4a2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.Tensor(\n",
    "    [(0.7,0.3, 1),\n",
    "     (0.4,0.5, 1),\n",
    "     (0.6,0.9, 1),\n",
    "     (0.2,0.2, 0)]\n",
    ")\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialise bias, weight1, weight2 \n",
    "initial_weights = torch.Tensor( (-0.5, 0.3, -0.2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ef060e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True : if the network fails to classify every sample in the data correctly.\n",
    "# Whenever it returns True, it will imply that our network requires more training.\n",
    "def keep_training(nn, data):\n",
    "    for sample in data:\n",
    "        if not torch.eq(nn(sample[0:2]), sample[-1]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa413246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to implement the learing algorithm of page 24 \n",
    "def train_perceptron(learning_rate, initial_weights, data):\n",
    "    perceptron = Net()# Gap 1\n",
    "    perceptron.fc1.bias.data = # Gap 2\n",
    "    perceptron.fc1.weight.data = # Gap 3\n",
    "    \n",
    "    \n",
    "    while keep_training(perceptron, data):\n",
    "        for sample in data:\n",
    "            temp_output = # Gap 4\n",
    "            label = sample[-1]\n",
    "            delta_w = # Gap 5\n",
    "            print(delta_w)\n",
    "            perceptron.fc1.bias.data = # Gap 6\n",
    "            perceptron.fc1.weight.data = # Gap 7\n",
    "            \n",
    "    return perceptron     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f92b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(learning_rate, initial_weights, data):\n",
    "    perceptron = Net()                     # Gap 1\n",
    "    \n",
    "    # initial_weights = (bias, w1, w2)\n",
    "    perceptron.fc1.bias.data = initial_weights[0:1]           # Gap 2\n",
    "    perceptron.fc1.weight.data = initial_weights[1:].view(1,2)  # Gap 3\n",
    "    \n",
    "    while keep_training(perceptron, data):\n",
    "        for sample in data:\n",
    "            x = sample[0:2]\n",
    "            label = sample[-1]\n",
    "            \n",
    "            temp_output = perceptron(x)                         # Gap 4\n",
    "            \n",
    "            error = label - temp_output\n",
    "            delta_w = learning_rate * error * x                 # Gap 5\n",
    "            print(delta_w)\n",
    "            \n",
    "            perceptron.fc1.bias.data += learning_rate * error   # Gap 6\n",
    "            perceptron.fc1.weight.data += delta_w.view(1,2)     # Gap 7\n",
    "            \n",
    "    return perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b67afb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0700, 0.0300], grad_fn=<MulBackward0>)\n",
      "tensor([0.0400, 0.0500], grad_fn=<MulBackward0>)\n",
      "tensor([0.0600, 0.0900], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([0.0400, 0.0500], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0200, -0.0200], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([0.0400, 0.0500], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0200, -0.0200], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us now run our algorithm, and see how far it goes in solving or classification problem!\n",
    "\n",
    "train_perceptron(0.1, initial_weights, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91df6dd",
   "metadata": {},
   "source": [
    "**Question:** *Compute the generalisation error of the perceptron trained above w.r.t. the ideal classification given in the above figure, i.e.  draw the classification boundary for the trained perceptron and compare it with that of the figure.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463166c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
