{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8f7bf7",
   "metadata": {},
   "source": [
    "## 1.Introduction to PyTorch, a Deep Learning Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062a44e",
   "metadata": {},
   "source": [
    "### 1-1. Tensors\n",
    "- Similar to array or matrix\n",
    "- Building block of neural networks \n",
    "##\n",
    "- Can be added or substracted, provided that their shapes are compatible\n",
    "- a * b : element-wise multiplication vs a @ b = Matrix multiplication <br>\n",
    "\n",
    "        → Perform addition and multiplication to process data and learn patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43dbc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[72, 75, 78],\n",
      "        [70, 73, 76]])\n"
     ]
    }
   ],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "\n",
    "temperatures = [[72, 75, 78], [70, 73, 76]] # can be created from Python lists or Numpy arrays\n",
    "\n",
    "# Create a tensor from temperatures\n",
    "temperatures= torch.tensor(temperatures)\n",
    "\n",
    "print(temperatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b4912",
   "metadata": {},
   "source": [
    "### 1-2. Cheking and adding tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0232f8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjustment shape: torch.Size([2, 3])\n",
      "Adjustment type: torch.int64\n",
      "Temperatures shape: torch.Size([2, 3])\n",
      "Temperatures type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "adjustment = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
    "\n",
    "# Display the shape of the adjustment tensor\n",
    "print(\"Adjustment shape:\", adjustment.shape)\n",
    "\n",
    "# Display the type of the adjustment tensor\n",
    "\n",
    "print(\"Adjustment type:\", adjustment.dtype)\n",
    "\n",
    "print(\"Temperatures shape:\", temperatures.shape)\n",
    "print(\"Temperatures type:\", temperatures.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27121fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected temperatures: tensor([[74, 77, 80],\n",
      "        [72, 75, 78]])\n"
     ]
    }
   ],
   "source": [
    "adjustment = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
    "\n",
    "# Add the temperatures and adjustment tensors\n",
    "corrected_temperatures = adjustment + temperatures\n",
    "print(\"Corrected temperatures:\", corrected_temperatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417216d",
   "metadata": {},
   "source": [
    "### 1-3. Neural networks and layers\n",
    "- A neural network consists of input(dataset features), hidden, and output layers(predictions).\n",
    "- ''Fully connected network''(= linear model) \n",
    "    - Every input neuron connects to every output neuron \n",
    "    - Network with no hidden layers where the output layer is a linear layer\n",
    "    - Linear layer : input @ wieghts + bias = output\n",
    "\n",
    "\n",
    "- weight : Reflects the importance of different features\n",
    "- bias : Provides the neuron with a baseline output \n",
    "\n",
    "ex) \n",
    "- input (temperature, humidity, wind) → output (rain, cloudy) <br>\n",
    "humidity feature will have a more significant weight<br>\n",
    "bias is to account for baseline information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1cc483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6501, 0.5737]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Linear layer network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]])\n",
    "\n",
    "# Create a Linear layer \n",
    "linear_layer = nn.Linear (\n",
    "    in_features= 3,\n",
    "    out_features= 2\n",
    ")\n",
    "\n",
    "# Pass input_tensor through the linear layer \n",
    "output = linear_layer(input_tensor)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048eb02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "766b39d6",
   "metadata": {},
   "source": [
    "### 1-4. Hidden layer and parameters\n",
    "- Stacking layes with nn.Seqential()\n",
    "\n",
    "Layer are made of neurons \n",
    "- Fully connected : each neuron links to all neurons in the previous layer \n",
    "- A neuron in a linear layer :\n",
    "    - perfroms a linear operation using all neurons from the previous layer \n",
    "    - Has N+1 parameters : N from inputs and 1 for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc831a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.0877]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Create a container for stacking linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8,4),\n",
    "    nn.Linear(4,1)\n",
    ") # cf) nn.Linear(input features, output features)\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb439052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2891,  0.1186,  0.0401, -0.0963, -0.1844, -0.0411, -0.1135,  0.2212,\n",
      "          0.2396],\n",
      "        [-0.1951,  0.1415, -0.2942,  0.0370,  0.1870, -0.0499, -0.2725, -0.0245,\n",
      "          0.1293],\n",
      "        [-0.2077,  0.0454, -0.1874,  0.2431, -0.0677, -0.0499,  0.3330,  0.3204,\n",
      "         -0.0576],\n",
      "        [ 0.2808,  0.3320,  0.0691, -0.0800,  0.0241, -0.0213,  0.2417,  0.1148,\n",
      "         -0.0924]], requires_grad=True)\n",
      "36\n",
      "Parameter containing:\n",
      "tensor([-0.2468,  0.1031, -0.1562,  0.0858], requires_grad=True)\n",
      "40\n",
      "Parameter containing:\n",
      "tensor([[ 0.0347,  0.0269, -0.0122, -0.2738],\n",
      "        [-0.0014, -0.1378, -0.3543, -0.4095]], requires_grad=True)\n",
      "48\n",
      "Parameter containing:\n",
      "tensor([-0.0818, -0.4063], requires_grad=True)\n",
      "50\n",
      "Parameter containing:\n",
      "tensor([[-0.0614, -0.4127]], requires_grad=True)\n",
      "52\n",
      "Parameter containing:\n",
      "tensor([-0.2353], requires_grad=True)\n",
      "53\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of parameters\n",
    "\n",
    "model = nn.Sequential(nn.Linear(9, 4),  # 9 * 4 / 1 * 4\n",
    "                      nn.Linear(4, 2),  # 2 * 4 / 1 * 2 \n",
    "                      nn.Linear(2, 1))  # 1 * 2 / 1 * 1\n",
    "\n",
    "total = 0\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)  # Weight & bias\n",
    "    total += parameter.numel()\n",
    "    print(total)\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3295ef",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d2363",
   "metadata": {},
   "source": [
    "## Week 1 review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157ca7a",
   "metadata": {},
   "source": [
    "###  Section 1: Implementing a simple linear NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8ad360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) : \n",
    "        super(Net, self).__init__()  # Initalise parent class\n",
    "        self.fc1 = nn.Linear(2,1) # Output layer \n",
    "\n",
    "    def forward(self, x):  # The function we specify which function to apply on the input \n",
    "        x = self.fc1(x)   # 입력값에 가중치 곱하고 편향 더해서 계산하는 것 \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6cd700d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.4788,  0.2652]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4565], requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = Net()\n",
    "\n",
    "list(net1.parameters())\n",
    "\n",
    "# requires_grad=True : 자동 미분 속성 \n",
    "# → Backpropagation 알고리즘을 실행할 경우, Pytorch가 이 매개변수들을 최적화(학습)할 것임을 의미한다\n",
    "# → 매개변수를 최적화 하려면 손실 값 (loss value)에 따른 기울기를 저장해야하는데 이 때 이 키워드가 필요한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ea0104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5412]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing whether nerons works\n",
    "x_input = torch.tensor([[0.1, 0.5]])\n",
    "net1(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bebf863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0.1* - 0.6601) + (0.5 * -0.0942) + -0.2797"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db67ec",
   "metadata": {},
   "source": [
    "### Section 2: Implementing a perceptron in PyTorch\n",
    "- 퍼셉트론 : 중간층(은닉층이 없음!) 오직 입력층과 아웃풋층 (step function을 activation function으로 사용함)\n",
    "- 이렇게 만든 퍼셉트론은 \"선형 분리\" 문제만 풀 수 있습니다. (예: 앤드(AND) 게이트, 오어(OR) 게이트)\n",
    "하지만 이 퍼셉트론 하나만으로는 절대 못 푸는 문제가 하나 있는데, 바로 유명한 XOR 문제입니다. 이 문제를 풀려면 아까 말씀하신 **'중간층(Hidden Layer)'**을 추가해야 하죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c005c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c978a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1) # Sum of input : 그 가중치랑 계산하는거 알지알지?\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) \n",
    "        # 1 if x > 0 else 0\n",
    "        # 활성화 여부를 결정함 (최종 결정의 스위치 역할 )\n",
    "        # 두번째 인자 values는 입력이 정확이 0 일때 어떤 값을 줄것인가를 계산함 (x>0, x<0 만 정의해서 x= 0일때 정의해줘야함 여기는 1로 정의)\n",
    "        # 요즘은 안씀. 미분이 불가해서 대신 가능한 sigmoid나 Lelu를 쓰는 것\n",
    "        x = torch.heaviside(x, torch.Tensor(1))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e04d8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        # Gap 1: single linear unit\n",
    "        self.fc1 = nn.Linear(2, 1)\n",
    "        \n",
    "        # Hardcoded weights and bias\n",
    "        with torch.no_grad(): # Pytorch는 기본적으로 모든 연산을 기록하기 때문에 no_grad = 계산 기록하지 마라 (그래프 생성 X) → backward 불가능 / ① 가중치 초기화 (지금 경우) ② 모델 평가 \n",
    "            self.fc1.weight = nn.Parameter(torch.tensor([[1.0, 1.0]]))\n",
    "            self.fc1.bias   = nn.Parameter(torch.tensor([-1.5]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Gap 2: Heaviside step function\n",
    "        x = (x > 0).float()\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc577cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_perceptron = Perceptron()\n",
    "my_perceptron.fc1.weight.data = torch.tensor([[ 0.4, 0.2]])\n",
    "my_perceptron.fc1.bias.data = torch.tensor([-0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e26636ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3000], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_perceptron.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "393541e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input [0.0, 0.0] -> Output 0\n",
      "Input [0.0, 1.0] -> Output 0\n",
      "Input [1.0, 0.0] -> Output 1\n",
      "Input [1.0, 1.0] -> Output 1\n"
     ]
    }
   ],
   "source": [
    "test_inputs = torch.tensor([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0]\n",
    "])\n",
    "\n",
    "outputs = my_perceptron(test_inputs)\n",
    "\n",
    "for x, y in zip(test_inputs, outputs):\n",
    "    #print(f\"Input {x} -> Output {int(y)}\") #tensor value (y는 또같은데...)\n",
    "    print(f\"Input {x.tolist()} -> Output {int(y.item())}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8bf49",
   "metadata": {},
   "source": [
    "## Section 3: Solving a classification problem using a Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa46a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1)\n",
    "        \n",
    "        self.fc1.weights = torch.nn.Parameter(torch.tensor([[0.66],[1]])) # 둘다 그림에서 계산한거 \n",
    "        self.fc1.bias = torch.nn.Parameter(torch.tensor([[-0.5]]))  \n",
    "        \n",
    "        self.heaviside = torch.heaviside\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        output = self.heavside(x, 0.5)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92318461",
   "metadata": {},
   "source": [
    "## Section 4: Training a perceptron\n",
    "- implement a method that trains the modelsuing some training data + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3808d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(0.7,0.3,1),(0.4,0.5,1), (0.6,0.9, 1), (0.2,0.2, 0), (0.1, 0.1, 0)]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.heaviside(x, torch.Tensor(1))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf4a2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.Tensor(\n",
    "    [(0.7,0.3, 1),\n",
    "     (0.4,0.5, 1),\n",
    "     (0.6,0.9, 1),\n",
    "     (0.2,0.2, 0)]\n",
    ")\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialise bias, weight1, weight2 \n",
    "initial_weights = torch.Tensor( (-0.5, 0.3, -0.2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef060e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True : if the network fails to classify every sample in the data correctly.\n",
    "# Whenever it returns True, it will imply that our network requires more training.\n",
    "def keep_training(nn, data):\n",
    "    for sample in data:\n",
    "        if not torch.eq(nn(sample[0:2]), sample[-1]) : #input values 2, answer 1 \n",
    "            # 모델 답하고 실제 답 비교해서 하나라도 다른게 있다면  \n",
    "            return True #계속 하세요\n",
    "    # # 6. 모든 문제를 다 돌았는데 틀린 게 하나도 없다면?\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa413246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to implement the learing algorithm of page 24 \n",
    "def train_perceptron(learning_rate, initial_weights, data):\n",
    "    perceptron = Net()# Gap 1\n",
    "    perceptron.fc1.bias.data = initial_weights[0]\n",
    "    perceptron.fc1.weight.data = initial_weights[1:]\n",
    "    \n",
    "    \n",
    "    while keep_training(perceptron, data):\n",
    "        for sample in data:\n",
    "            temp_output = perceptron(sample[0:2])\n",
    "            label = sample[-1]\n",
    "            delta_w = learning_rate * (label - temp_output) * torch.Tensor( [1, sample[0], sample[1]] )\n",
    "            print(delta_w)\n",
    "            perceptron.fc1.bias.data = perceptron.fc1.bias.data + delta_w[0]\n",
    "            perceptron.fc1.weight.data = perceptron.fc1.weight.data + delta_w[1:]\n",
    "            \n",
    "    return perceptron     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(learning_rate, initial_weights, data):\n",
    "    perceptron = Net()                     # Gap 1\n",
    "    \n",
    "    # initial_weights = (bias, w1, w2)\n",
    "    perceptron.fc1.bias.data = initial_weights[0:1]           # Gap 2\n",
    "    perceptron.fc1.weight.data = initial_weights[1:].view(1,2)  # ⭐️ Gap 3\n",
    "    \n",
    "    while keep_training(perceptron, data):\n",
    "        for sample in data:\n",
    "            x = sample[0:2]\n",
    "            label = sample[-1]\n",
    "            \n",
    "            temp_output = perceptron(x)                         # Gap 4\n",
    "            \n",
    "            error = label - temp_output\n",
    "            delta_w = learning_rate * error * x                 # Gap 5 (직관적으로 이해하면 원래 x 에서 learning_rate * loss ) 업데이트 !\n",
    "            print(delta_w) \n",
    "            \n",
    "            perceptron.fc1.bias.data += learning_rate * error   # Gap 6\n",
    "            perceptron.fc1.weight.data += delta_w.view(1,2)     # ⭐️ Gap 7 (1행 2열 벡터로 만듬)\n",
    "            \n",
    "    return perceptron # We return the perceptron so we can use the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b67afb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0700, 0.0300], grad_fn=<MulBackward0>)\n",
      "tensor([0.0400, 0.0500], grad_fn=<MulBackward0>)\n",
      "tensor([0.0600, 0.0900], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([0.0400, 0.0500], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0200, -0.0200], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([0.0400, 0.0500], grad_fn=<MulBackward0>)\n",
      "tensor([0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0200, -0.0200], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us now run our algorithm, and see how far it goes in solving or classification problem!\n",
    "\n",
    "train_perceptron(0.1, initial_weights, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91df6dd",
   "metadata": {},
   "source": [
    "**Question:** *Compute the generalisation error of the perceptron trained above w.r.t. the ideal classification given in the above figure, i.e.  draw the classification boundary for the trained perceptron and compare it with that of the figure.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463166c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
